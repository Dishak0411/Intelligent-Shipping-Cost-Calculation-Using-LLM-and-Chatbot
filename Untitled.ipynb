{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4911047f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForCausalLM, AutoTokenizer\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5b2b76",
   "metadata": {},
   "source": [
    "# Loading the GPT-2 model in tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a96438a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3132cec3",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "599591d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Product  Length  Breadth  Height  Weight\n",
      "0              Chair      20       20      30       5\n",
      "1              Table      60       40      30      15\n",
      "2     BILLY Bookcase      80       28     202      25\n",
      "3     MALM Bed Frame     209      177      38      32\n",
      "4  LACK Coffee Table     118       78      45      12\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('Furniture_Dataset.csv')\n",
    "df.columns = df.columns.str.strip()\n",
    "print(df.head())\n",
    "data = []\n",
    "for index, row in df.iterrows():\n",
    "    input_text = f\"Calculate shipping cost for {row['Product']} with dimensions {row['Length']}x{row['Breadth']}x{row['Height']} cm and weight {row['Weight']} kg.\"\n",
    "    output_text = \"Shipping cost is $50\"\n",
    "    data.append({\"input\": input_text, \"label\" : output_text})\n",
    "\n",
    "transformed_df = pd.DataFrame(data)\n",
    "transformed_df.to_csv('training_data.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd25f439",
   "metadata": {},
   "source": [
    "# Tokenization and Data Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47b829ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Mixed precision compatibility check (mixed_float16): WARNING\n",
      "The dtype policy mixed_float16 may run slowly because this machine does not have a GPU. Only Nvidia GPUs with compute capability of at least 7.0 run quickly with mixed_float16.\n",
      "If you will use compatible GPU(s) not attached to this host, e.g. by running a multi-worker model, you can ignore this warning. This message will only be logged once\n",
      "Epoch 1/3\n",
      "6/6 [==============================] - 350s 49s/step - loss: 8.4849\n",
      "Epoch 2/3\n",
      "6/6 [==============================] - 271s 44s/step - loss: 7.0560\n",
      "Epoch 3/3\n",
      "6/6 [==============================] - 284s 46s/step - loss: 4.2484\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_gpt2\\\\tokenizer_config.json',\n",
       " './fine_tuned_gpt2\\\\special_tokens_map.json',\n",
       " './fine_tuned_gpt2\\\\vocab.json',\n",
       " './fine_tuned_gpt2\\\\merges.txt',\n",
       " './fine_tuned_gpt2\\\\added_tokens.json',\n",
       " './fine_tuned_gpt2\\\\tokenizer.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TFAutoModelForCausalLM, AutoTokenizer, create_optimizer\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = TFAutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Ensure tokenizer has a pad token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "tokenizer.pad_token = tokenizer.convert_tokens_to_ids('[PAD]')\n",
    "\n",
    "# Load and preprocess the training data\n",
    "df = pd.read_csv('training_data.csv')\n",
    "inputs = df['input'].tolist()\n",
    "labels = df['label'].tolist()\n",
    "\n",
    "# Tokenize inputs and labels\n",
    "max_length = 512  # Adjust max_length as needed\n",
    "tokenized_inputs = tokenizer(inputs, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"tf\")\n",
    "tokenized_labels = tokenizer(labels, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"tf\")\n",
    "\n",
    "# Convert tensors to a format compatible with TensorFlow Dataset\n",
    "input_ids = tokenized_inputs['input_ids']\n",
    "attention_mask = tokenized_inputs['attention_mask']\n",
    "labels = tokenized_labels['input_ids']\n",
    "\n",
    "# Create TensorFlow dataset with reduced batch size\n",
    "def create_tf_dataset(input_ids, attention_mask, labels, batch_size=4):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((\n",
    "        {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask\n",
    "        }, labels\n",
    "    ))\n",
    "    \n",
    "    # Calculate maximum sequence length for padding\n",
    "    max_length = input_ids.shape[1]\n",
    "\n",
    "    # Pad the batches to ensure consistent shapes\n",
    "    dataset = dataset.padded_batch(\n",
    "        batch_size,\n",
    "        padded_shapes=({\n",
    "            'input_ids': [max_length],\n",
    "            'attention_mask': [max_length]\n",
    "        }, [max_length]),\n",
    "        padding_values=({\n",
    "            'input_ids': tokenizer.pad_token_id,\n",
    "            'attention_mask': 0\n",
    "        }, tokenizer.pad_token_id)\n",
    "    )\n",
    "    \n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "train_dataset = create_tf_dataset(input_ids, attention_mask, labels, batch_size=4)\n",
    "\n",
    "# Define a custom loss function\n",
    "def custom_loss(y_true, y_pred):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)\n",
    "\n",
    "# Create a Keras optimizer\n",
    "optimizer, _ = create_optimizer(init_lr=5e-5, num_train_steps=len(df)//4, num_warmup_steps=100)\n",
    "\n",
    "# Enable mixed precision training\n",
    "from tensorflow.keras import mixed_precision\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)\n",
    "\n",
    "# Compile the model with the optimizer and custom loss function\n",
    "model.compile(optimizer=optimizer, loss=custom_loss)\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_dataset, epochs=3)\n",
    "\n",
    "# Save the model and tokenizer\n",
    "model.save_pretrained('./fine_tuned_gpt2')\n",
    "tokenizer.save_pretrained('./fine_tuned_gpt2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c66194e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_gpt2\\\\tokenizer_config.json',\n",
       " './fine_tuned_gpt2\\\\special_tokens_map.json',\n",
       " './fine_tuned_gpt2\\\\vocab.json',\n",
       " './fine_tuned_gpt2\\\\merges.txt',\n",
       " './fine_tuned_gpt2\\\\added_tokens.json',\n",
       " './fine_tuned_gpt2\\\\tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForCausalLM\n",
    "\n",
    "# Load the pre-trained model and tokenizer\n",
    "model = TFAutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Save the model and tokenizer\n",
    "model.save_pretrained('./fine_tuned_gpt2')\n",
    "tokenizer.save_pretrained('./fine_tuned_gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "198ed399",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('./fine_tuned_gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd6dbc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_shipping_cost(length, breadth, height, weight, pincode):\n",
    "    base_rate = 50\n",
    "    metro_pincodes = ['1001', '1002', '2001']\n",
    "    same_city_pincodes = ['1000']\n",
    "\n",
    "    is_metro = pincode in metro_pincodes\n",
    "    is_same_city = pincode in same_city_pincodes\n",
    "\n",
    "    height_surcharge = 20 if height > 60 else 0\n",
    "    weight_surcharge = 15 if weight > 8 else 0\n",
    "\n",
    "    distance_factor = 0 if is_same_city else (30 if is_metro else 50)\n",
    "\n",
    "    total_cost = base_rate + height_surcharge + weight_surcharge + distance_factor\n",
    "    return total_cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "485f1db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at ./fine_tuned_gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [06/Sep/2024 01:27:53] \"POST /get_shipping_cost HTTP/1.1\" 400 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from transformers import TFAutoModelForCausalLM, AutoTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model = TFAutoModelForCausalLM.from_pretrained('./fine_tuned_gpt2')\n",
    "tokenizer = AutoTokenizer.from_pretrained('./fine_tuned_gpt2')\n",
    "\n",
    "def load_pincodes(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return set(line.strip() for line in file)\n",
    "\n",
    "def generate_shipping_cost(query):\n",
    "    input_ids = tokenizer.encode(query, return_tensors='tf')\n",
    "    output = model.generate(input_ids, max_length=50, num_return_sequences=1)\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "metro_pincodes = load_pincodes('metro_pincodes.txt')\n",
    "same_city_pincodes = load_pincodes('same_city_pincodes.txt')\n",
    "undeliverable_pincodes = ['110080','110090','700104','700112','400104','600098','412404','412407']\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return \"Welcome to the Shipping Cost Calculator API!\"\n",
    "\n",
    "@app.route('/get_shipping_cost', methods=['POST'])\n",
    "def get_shipping_cost():\n",
    "    try:\n",
    "        data = request.get_json()\n",
    "#         print(\"Received data :\", data)\n",
    "        product_name = data.get('product_name')\n",
    "#         print(\"Product_name : \", product_name)\n",
    "        pincode = data.get('pincode')\n",
    "#         print(\"pincode : \", pincode)\n",
    "        \n",
    "        # Load product details\n",
    "        df = pd.read_csv('furniture_dataset.csv')\n",
    "        product = df[df['Product'] == product_name]\n",
    "\n",
    "        if product.empty:\n",
    "            return jsonify({'error': 'Product not found'}), 404\n",
    "        \n",
    "        # Check if the pincode is deliverable\n",
    "        if pincode in undeliverable_pincodes:\n",
    "            return jsonify({'error':'Delivery is not available to this pincode'}), 400\n",
    "        \n",
    "        # Calculate shipping cost\n",
    "        dimensions = product[['Length', 'Breadth', 'Height']].values[0]\n",
    "        weight = product['Weight'].values[0]\n",
    "\n",
    "        # Sample cost calculation logic\n",
    "        base_shipping_cost = 50  # Base cost\n",
    "        additional_cost = 0\n",
    "\n",
    "        # Add extra cost for oversized items\n",
    "        if dimensions[2] > 2 * 30.48 or weight > 8:  # 2 feet in centimeters\n",
    "            additional_cost += 20  # Additional charge for oversized items\n",
    "\n",
    "        # Check if the pincode is in a metro city\n",
    "        if pincode in metro_pincodes:\n",
    "            shipping_cost = base_shipping_cost  # Lower cost for metro cities\n",
    "        else:\n",
    "            shipping_cost = base_shipping_cost + 30  # Higher charge for non-metro cities\n",
    "\n",
    "        total_cost = shipping_cost + additional_cost\n",
    "\n",
    "        return jsonify({'shipping_cost': total_cost})\n",
    "    \n",
    "    except Exception as e:\n",
    "        return jsonify({'error': str(e)}), 500\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        app.run()\n",
    "    except KeyboardInterrupt:\n",
    "        sys.exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6694971c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter 'exit' to quit.\n",
      "Enter product name:\n",
      "chair\n",
      "Enter pincode:\n",
      "411043\n",
      "Error: Product not found\n",
      "Enter 'exit' to quit.\n",
      "Enter product name:\n",
      "exit\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def get_shipping_cost_from_flask(product_name, pincode):\n",
    "    url = 'http://127.0.0.1:5000/get_shipping_cost'\n",
    "    payload = {'product_name': product_name, 'pincode': pincode}\n",
    "    response = requests.post(url, json=payload)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json().get('shipping_cost')\n",
    "    else:\n",
    "        return response.json().get('error')\n",
    "\n",
    "def chat():\n",
    "    while True:\n",
    "        print(\"Enter 'exit' to quit.\")\n",
    "        print(\"Enter product name:\")\n",
    "        product_name = input()\n",
    "        \n",
    "        if product_name.lower() == 'exit':\n",
    "            break\n",
    "        \n",
    "        print(\"Enter pincode:\")\n",
    "        pincode = input()\n",
    "        \n",
    "        if pincode.lower() == 'exit':\n",
    "            break\n",
    "\n",
    "        shipping_cost = get_shipping_cost_from_flask(product_name, pincode)\n",
    "        \n",
    "        if isinstance(shipping_cost, str):  # Error message\n",
    "            print(f\"Error: {shipping_cost}\")\n",
    "        else:\n",
    "            print(f\"The shipping cost is: ${shipping_cost}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    chat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fcb790",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
